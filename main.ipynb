{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39044b26-84bb-4854-9492-d018ad20b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "from scipy import signal\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08723ce2-b586-4f78-bfc9-1845b4c580a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2196b-8971-4718-886a-d033209d0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1ce35-0707-43d9-8dd5-db81f4655a2f",
   "metadata": {},
   "source": [
    "Thankfully no Nulls in data.\n",
    "Below heatmaps for Geospatial density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd640349-335e-4044-9d75-36b18d4551ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_heatmap(train_df, x=\"longitude\", y=\"latitude\", marginal_x=\"histogram\", marginal_y=\"histogram\", width=700, height=700, nbinsx=100, nbinsy=100)\n",
    "fig.show(filename='basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3899d2-afdb-4eb3-bb66-085997c7da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_heatmap(test_df, x=\"longitude\", y=\"latitude\", marginal_x=\"histogram\", marginal_y=\"histogram\", width=700, height=700, nbinsx=100, nbinsy=100)\n",
    "fig.show(filename='basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc77b0-5425-4671-a8aa-2fae48b38b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(train_df,\n",
    "                        lat=\"latitude\",\n",
    "                        lon=\"longitude\",\n",
    "                        zoom=4,\n",
    "                        color=\"label\",\n",
    "                        height=900,\n",
    "                        width=1300)\n",
    "fig.update_layout(mapbox_style='carto-positron')\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152ceb1-9537-4393-860d-a0d08b5921e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_df, y_vars='label')\n",
    "sns.pairplot(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb756283-b9a2-4503-a55f-9c2aa2e1101f",
   "metadata": {},
   "source": [
    "Checking if labels are correlated to lat/long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f7041-f24d-4140-b987-d274ed923775",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5), sharey=True)\n",
    "n_labels = len(np.unique(train_df['label']))\n",
    "for index, label in enumerate(np.unique(train_df['label'])):\n",
    "    data_ = train_df[train_df['label']==label]\n",
    "    title = 'Class ' + str(int(label))\n",
    "    sns.histplot(data=data_, x=\"longitude\", y=\"latitude\", cbar=True, bins=50, ax=axes[index]).set(title=title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5034e-7f5f-4fb7-9e8e-e57ce2ea3b08",
   "metadata": {},
   "source": [
    "Checking if train/test split is proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca124b8f-07b0-4d4b-a3bf-aba189a94ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_df['longitude'], train_df['latitude'])\n",
    "plt.scatter(test_df['longitude'], test_df['latitude'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cab964-c804-4fdb-89de-471da8a2dcb2",
   "metadata": {},
   "source": [
    "Equalizing histograms for y_CrCb and HSV color space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36100bb-4db8-4451-9eaa-ddb2927f3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hisEqulColor(img):\n",
    "    ycrcb=cv2.cvtColor(img,cv2.COLOR_BGR2YCR_CB)\n",
    "    channels=cv2.split(ycrcb)\n",
    "    hist = cv2.equalizeHist(channels[0])\n",
    "    ycrcb = cv2.merge((hist,  channels[1], channels[2]))\n",
    "    img = cv2.cvtColor(ycrcb,cv2.COLOR_YCR_CB2BGR)\n",
    "    return img\n",
    "\n",
    "def hisEqulColorHSV(img):\n",
    "    hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "    channels=cv2.split(hsv)\n",
    "    hist = cv2.equalizeHist(channels[2])\n",
    "    hsv = cv2.merge((channels[0],  channels[1], hist))\n",
    "    img = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a3102d-2d48-4351-a1c6-c844970f3394",
   "metadata": {},
   "source": [
    "Combining 2 Sobel filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ce7e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def combined_filter(img, mask1, mask2):\n",
    "    conv1 = np.zeros(img.shape)\n",
    "    conv2 = np.zeros(img.shape)\n",
    "    for i in range(3):\n",
    "        conv1[:, :, i] = signal.convolve2d(img[:, :, i], mask1, mode='same', boundary='symm')\n",
    "        conv2[:, :, i] = signal.convolve2d(img[:, :, i], mask2, mode='same', boundary='symm')\n",
    "    return np.sqrt(conv1**2 + conv2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64435b13-71b4-4649-b14c-fad21de034ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_classes = [train_df[train_df['label']==0], train_df[train_df['label']==1], train_df[train_df['label']==2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb06bf4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s1 = np.array([[-1,0,1],[-2,0,2],[-1,0,1]])\n",
    "s2 = np.array([[-1,-2,-1],[0,0,0],[1,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149d3c6-0a8a-4944-beb3-047b771c5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_rec(class_n):\n",
    "    n_pics = 4\n",
    "    paths = sep_classes[class_n]['example_path'].sample(n_pics).to_list()\n",
    "    fig, ax = plt.subplots(n_pics, 5)\n",
    "    fig.set_size_inches(15,12)\n",
    "    for i, path in enumerate(paths):\n",
    "        img_ = cv2.imread(path)\n",
    "        img_ycrcb = hisEqulColor(img_)\n",
    "        img_hsv = hisEqulColorHSV(img_)\n",
    "        ax[i, 0].imshow(img_)\n",
    "        ax[i, 1].imshow(img_ycrcb)\n",
    "        ax[i, 2].imshow(img_hsv)\n",
    "        ax[i, 3].imshow(combined_filter(img_ycrcb/255,s1,s2))\n",
    "        ax[i, 4].imshow(combined_filter(img_hsv/255,s1,s2))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec20de9-f75a-4a47-88d3-fa8274fb1593",
   "metadata": {},
   "source": [
    "Seeing how filters work on different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6382f1-d1fb-4242-914b-77b42e0e0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_rec(0)\n",
    "edge_rec(1)\n",
    "edge_rec(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b60c6e-41ac-4969-b87a-3441b16aaae4",
   "metadata": {},
   "source": [
    "Preparing data for some simple naive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0a3fa-e3a9-4752-b31f-fcd663ef521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(train_df['label']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4a7e3-c02c-44d4-93e7-8bb85843f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_df[['latitude','longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe6a3d-2ab3-4cd7-8be6-23274496904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f612f-695e-4b2f-af2c-0254e532fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_depth = 3\n",
    "tree_clf = DecisionTreeClassifier(max_depth=tree_depth, random_state=42)\n",
    "tree_clf.fit(x_train,y_train)\n",
    "\n",
    "f1tr = f1_score(y_train,tree_clf.predict(x_train), average='weighted')\n",
    "f1te = f1_score(y_test,tree_clf.predict(x_test), average='weighted')\n",
    "acctr = accuracy_score(y_train,tree_clf.predict(x_train))\n",
    "accte = accuracy_score(y_test,tree_clf.predict(x_test))\n",
    "\n",
    "print(f1tr, f1te, acctr, accte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bbda07-37b7-4de2-ab65-2e2cf8ab6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=10)\n",
    "rnd_clf.fit(x_train,y_train)\n",
    "\n",
    "f1tr = f1_score(y_train,rnd_clf.predict(x_train), average='weighted')\n",
    "f1te = f1_score(y_test,rnd_clf.predict(x_test), average='weighted')\n",
    "acctr = accuracy_score(y_train,rnd_clf.predict(x_train))\n",
    "accte = accuracy_score(y_test,rnd_clf.predict(x_test))\n",
    "\n",
    "print(f1tr, f1te, acctr, accte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d6ef6-acea-4eb7-8fc2-2359f1d1722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_clf = GradientBoostingClassifier(n_estimators=10)\n",
    "gbc_clf.fit(x_train,y_train)\n",
    "\n",
    "f1tr = f1_score(y_train,gbc_clf.predict(x_train), average='weighted')\n",
    "f1te = f1_score(y_test,gbc_clf.predict(x_test), average='weighted')\n",
    "acctr = accuracy_score(y_train,gbc_clf.predict(x_train))\n",
    "accte = accuracy_score(y_test,gbc_clf.predict(x_test))\n",
    "\n",
    "print(f1tr, f1te, acctr, accte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e72656",
   "metadata": {},
   "source": [
    "Read in train pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21048be2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_paths_per_class = train_df.groupby(\"label\")[\"example_path\"].apply(list).to_dict()\n",
    "images_per_class = {\n",
    "    label: [cv2.imread(path) for path in paths]\n",
    "    for label, paths in image_paths_per_class.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5241b",
   "metadata": {},
   "source": [
    "Let's see the distribution of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5478fb42",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "{label: len(images) for label, images in images_per_class.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f1593",
   "metadata": {},
   "source": [
    "We will need to augment the data in order to make every class representative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7503cdae",
   "metadata": {},
   "source": [
    "Read in test pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a04ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_pictures_paths = test_df[\"example_path\"].tolist()\n",
    "test_images = [cv2.imread(path) for path in test_pictures_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Converting data to proper formats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = []\n",
    "data = []\n",
    "for label, pictures in images_per_class.items():\n",
    "    labels.extend([label] * len(pictures))\n",
    "    data.extend(pictures)\n",
    "\n",
    "labels = tf.one_hot(labels, 3).numpy()\n",
    "data = np.array(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Splitting the data into train and validation sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(data, labels, test_size=0.2)\n",
    "print(len(y_valid[y_valid[:, 0] == 1]), len(y_valid[y_valid[:, 1] == 1]), len(y_valid[y_valid[:, 2] == 1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's scale the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_unprocessed = X_train / 255\n",
    "X_valid_unprocessed = X_valid / 255"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train model on unprocessed data to have comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_unprocessed = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=(332, 332, 3)),\n",
    "        keras.layers.Conv2D(16, (3,3)),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Conv2D(16, (3,3)),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Conv2D(16, (5,5)),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dense(64, activation=\"leaky_relu\"),\n",
    "        keras.layers.Dense(8, activation=\"leaky_relu\"),\n",
    "        keras.layers.Dense(3, activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "model_unprocessed.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_unprocessed.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", recall_m, precision_m, f1_m])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model_unprocessed.fit(X_train_unprocessed, y_train, validation_data=(X_valid_unprocessed, y_valid), epochs=15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_model_performance(history):\n",
    "    # extract loss and accuracy of model\n",
    "    model_loss = history[\"loss\"]\n",
    "    model_val_loss = history[\"val_loss\"]\n",
    "    model_acc = history[\"accuracy\"]\n",
    "    model_val_acc = history[\"val_accuracy\"]\n",
    "    model_recall = history[\"recall_m\"]\n",
    "    model_val_recall = history[\"val_recall_m\"]\n",
    "    model_precission = history[\"precision_m\"]\n",
    "    model_val_precission = history[\"val_precision_m\"]\n",
    "    model_f1 = history[\"f1_m\"]\n",
    "    model_val_f1 = history[\"val_f1_m\"]\n",
    "    epochs = range(len(model_loss))\n",
    "\n",
    "    # create subplot\n",
    "    plt.clf()\n",
    "    fig, axs = plt.subplots(5, 1, sharex=True)\n",
    "    fig.set_size_inches(10, 10)\n",
    "\n",
    "    # loss\n",
    "    axs[0].plot(epochs, model_loss, 'b', label='Training loss')\n",
    "    axs[0].plot(epochs, model_val_loss, 'r', label='Validation loss')\n",
    "    axs[0].title.set_text('Training and validation loss')\n",
    "    axs[0].set(ylabel=\"Loss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # accuracy\n",
    "    axs[1].plot(epochs, model_acc, 'b', label='Training accuracy')\n",
    "    axs[1].plot(epochs, model_val_acc, 'r', label='Validation accuracy')\n",
    "    axs[1].title.set_text('Training and validation accuracy')\n",
    "    axs[1].set(ylabel=\"Accuracy\")\n",
    "    axs[1].legend()\n",
    "    axs[1].set_ylim([0, 1])\n",
    "\n",
    "     # recall\n",
    "    axs[2].plot(epochs, model_recall, 'b', label='Training recall')\n",
    "    axs[2].plot(epochs, model_val_recall, 'r', label='Validation recall')\n",
    "    axs[2].title.set_text('Training and validation recall')\n",
    "    axs[2].set(ylabel=\"Recall\")\n",
    "    axs[2].legend()\n",
    "    axs[2].set_ylim([0, 1])\n",
    "\n",
    "    # precission\n",
    "    axs[3].plot(epochs, model_precission, 'b', label='Training precision')\n",
    "    axs[3].plot(epochs, model_val_precission, 'r', label='Validation precision')\n",
    "    axs[3].title.set_text('Training and validation precision')\n",
    "    axs[3].set(ylabel=\"Precision\")\n",
    "    axs[3].legend()\n",
    "    axs[3].set_ylim([0, 1])\n",
    "\n",
    "    # f1\n",
    "    axs[4].plot(epochs, model_f1, 'b', label='Training f1 score')\n",
    "    axs[4].plot(epochs, model_val_f1, 'r', label='Validation f1 score')\n",
    "    axs[4].title.set_text('Training and validation f1 score')\n",
    "    axs[4].set(ylabel=\"F1 score\")\n",
    "    axs[4].legend()\n",
    "    axs[4].set_ylim([0, 1])\n",
    "\n",
    "    fig.text(0.5, 0.04, 'Epochs', ha='center')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_model_performance(history.history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "784f25e2",
   "metadata": {},
   "source": [
    "\n",
    "Preprocess data using hsv and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355384a-afd3-40f4-b215-d48e0277865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_picture(img):\n",
    "    img_hsv_normalized = hisEqulColorHSV(img)\n",
    "    filtered_hsv = combined_filter(img_hsv_normalized/255,s1,s2)\n",
    "    return filtered_hsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_preprocessed = np.array([preprocess_picture(img) for img in X_train])\n",
    "X_valid_preprocessed = np.array([preprocess_picture(img) for img in X_valid])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "00f787b9",
   "metadata": {},
   "source": [
    "Creating new model that will be trained on preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0b166",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_processed = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=(332, 332, 3)),\n",
    "        keras.layers.Conv2D(16, (3,3)),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Conv2D(16, (3,3)),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Conv2D(16, (5,5)),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dense(64, activation=\"leaky_relu\"),\n",
    "        keras.layers.Dense(8, activation=\"leaky_relu\"),\n",
    "        keras.layers.Dense(3, activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "model_processed.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfecf465",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_processed.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", recall_m, precision_m, f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c0c30f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "history_processed = model_unprocessed.fit(X_train_preprocessed, y_train, validation_data=(X_valid_preprocessed, y_valid), epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b63999",
   "metadata": {},
   "source": [
    "Plotting performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b8fed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_model_performance(history_processed.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we see the model did not perform better than the base one, let's try to increase size of the kernels, shrink down the linear layers size and add dropout as we have seen that model is overfitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_processed_2 = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=(332, 332, 3)),\n",
    "        keras.layers.Conv2D(16, (5,5)),\n",
    "        keras.layers.MaxPool2D(),\n",
    "        keras.layers.Conv2D(16, (5, 5)),\n",
    "        keras.layers.MaxPool2D(),\n",
    "        keras.layers.Conv2D(16, (3,3)),\n",
    "        keras.layers.MaxPool2D(),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(64, activation=\"leaky_relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(8, activation=\"leaky_relu\"),\n",
    "        keras.layers.Dense(3, activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "model_processed_2.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "42c8d2cc",
   "metadata": {},
   "source": [
    "As we have seen previously the data need to be augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce192bbb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_processed_2.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", recall_m, precision_m, f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history_processed_2 = model_processed_2.fit(X_train_preprocessed, y_train, validation_data=(X_valid_preprocessed, y_valid), epochs=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_model_performance(history_processed_2.history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "777f0ee9",
   "metadata": {},
   "source": [
    "It seems that bigger model did not solve our issue - it shows huge overfitting, let's try keeping the kernel size and let's shrink amount and size of Dense layers and change their activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_processed_3 = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=(332, 332, 3)),\n",
    "        keras.layers.Conv2D(16, (5,5)),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Conv2D(12, (3, 3), activation=\"leaky_relu\"),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Conv2D(8, (3,3), activation=\"leaky_relu\"),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(256, activation=\"leaky_relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(32, activation=\"tanh\"),\n",
    "        keras.layers.Dense(3, activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "model_processed_3.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_processed_3.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", recall_m, precision_m, f1_m])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history_processed_3 = model_processed_3.fit(X_train_preprocessed, y_train, validation_data=(X_valid_preprocessed, y_valid), epochs=15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_model_performance(history_processed_3.history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Augmenting the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b746335",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "per_class_counts = {\n",
    "    label: len(y_train[y_train[:, label] == 1])\n",
    "    for label in range(3)\n",
    "}\n",
    "max_count = max(per_class_counts.values())\n",
    "deficit = {label: abs(per_class_counts[label] - max_count) for label in per_class_counts}\n",
    "added_labels, added_images = [], []\n",
    "for label, class_deficit in deficit.items():\n",
    "    pool_of_images = X_train_preprocessed[y_train[:,label] == 1]\n",
    "    indices = np.random.choice(np.arange(len(pool_of_images)), deficit[label]).astype(np.int32)\n",
    "    if len(indices):\n",
    "        images_to_add = pool_of_images[indices]\n",
    "        added_labels.extend([label] * class_deficit)\n",
    "        added_images.append(images_to_add)\n",
    "\n",
    "added_images = np.vstack(added_images)\n",
    "added_labels = tf.one_hot(added_labels, 3)\n",
    "X_train_preprocessed = np.vstack([X_train_preprocessed, added_images])\n",
    "y_train = np.vstack([y_train, added_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_augmented = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=(332, 332, 3)),\n",
    "        keras.layers.Conv2D(16, (5,5)),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Conv2D(10, (4, 4), activation=\"leaky_relu\"),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Conv2D(6, (3,3), activation=\"leaky_relu\"),\n",
    "        keras.layers.MaxPool2D((3,3)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation=\"leaky_relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(32, activation=\"tanh\"),\n",
    "        keras.layers.Dense(3, activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "model_augmented.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_augmented.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", recall_m, precision_m, f1_m])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history_augmented = model_augmented.fit(X_train_preprocessed, y_train, validation_data=(X_valid_preprocessed, y_valid), epochs=15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_model_performance(history_augmented.history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chosen_model = model_unprocessed\n",
    "paths = test_df[\"example_path\"].tolist()\n",
    "ids = [path.split('/')[2][:-4] for path in paths]\n",
    "test_images_np = np.array(test_images)\n",
    "test_images_scaled = test_images_np / 255\n",
    "result_dict = {\"target\": {}}\n",
    "predictions = chosen_model.predict(test_images_scaled)\n",
    "predictions_int = np.argmax(predictions, axis=1)\n",
    "for idx, prediction in zip(ids, predictions_int):\n",
    "    result_dict[\"target\"][idx] = int(prediction)\n",
    "\n",
    "with open(\"predictions.json\", \"w+\") as fout:\n",
    "    json.dump(result_dict, fout)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
